{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.activations import softmax\n",
    "def self(input_sequence):\n",
    "    output = np.zeros(input_sequence.shape)\n",
    "    for i, pivot_vector in enumerate(input_sequence):\n",
    "        scores = np.zeros(shape=len(input_sequence))\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            scores[j] = np.dot(pivot_vector, vector.T)\n",
    "        scores /= np.sqrt(input_sequence.shape[1])\n",
    "        scores = softmax(scores)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "max_tokens = 20000\n",
    "max_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim':self.embed_dim,\n",
    "            'num_heads':self.num_heads,\n",
    "            'dense_dim':self.dense_dim\n",
    "        })\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def layer_normalization(batch_of_sequences):\n",
    "    mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n",
    "    variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n",
    "    return (batch_of_sequences - mean) / variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def batch_normalization(batch_of_images):\n",
    "    mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
    "    variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
    "    return (batch_of_images - mean) / variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'aclImdb/train'\n",
    "validation_dir = 'aclImdb/val'\n",
    "test_dir = 'aclImdb/test'\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "validation_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    directory=validation_dir,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    directory=test_dir,\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization, Embedding\n",
    "train_ds_text_only = train_ds.map(lambda x, y:x)\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vectorization.adapt(train_ds_text_only)\n",
    "int_train_ds = train_ds.map(lambda x, y:(text_vectorization(x), y))\n",
    "int_validation_ds = validation_ds.map(lambda x, y:(text_vectorization(x), y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.7692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, multi_head_attention_12_layer_call_fn, multi_head_attention_12_layer_call_and_return_conditional_losses, layer_normalization_24_layer_call_fn, layer_normalization_24_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Transformer model - text classification models\\model_1_transformer_encoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Transformer model - text classification models\\model_1_transformer_encoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 472s 754ms/step - loss: 0.4952 - accuracy: 0.7692 - val_loss: 0.3612 - val_accuracy: 0.8378\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "inputs = layers.Input(shape=(None,), dtype='int64')\n",
    "# x = text_vectorization(inputs)\n",
    "x = Embedding(input_dim=max_tokens, input_length=max_length, output_dim=embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim,dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_1 = keras.models.Model(inputs, outputs)\n",
    "\n",
    "model_1.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint('Transformer model - text classification models/model_1_transformer_encoder',\n",
    "                                                save_best_only=True)]\n",
    "\n",
    "history_model_1 = model_1.fit(\n",
    "    int_train_ds,\n",
    "    epochs=1,\n",
    "    validation_data=int_validation_ds,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_12 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder_12 (Tra  (None, None, 256)        543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        position = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_position = self.position_embeddings(position)\n",
    "        return embedded_tokens + embedded_position\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'input_dim':self.input_dim\n",
    "        })\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "import keras\n",
    "inputs = layers.Input(shape=(None,), dtype='int64')\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim,dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_2 = keras.models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model_2.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    'Transformer model - text classification models/model_2_transformer_encoder_positional_embeddings',\n",
    "    save_best_only=True\n",
    ")]\n",
    "\n",
    "# history_model_2 = model_2.fit(\n",
    "#     int_train_ds,\n",
    "#     epochs=5,\n",
    "#     validation_data=int_validation_ds,\n",
    "#     callbacks=callbacks\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "[0.9776884] of positivity\n"
     ]
    }
   ],
   "source": [
    "token = text_vectorization([\"it is a glorious film, but you could not make it now. and that is not just my opinion my preview screening companion and fellow lover of weepy old black and white films agreed that not even the director max ophuls could get away with this 1948 classic if he tried to make it in 2009 but why not well for start it is visually sumptomps but in black and white if you remade it in clour you can never mathc But why not? Well, for a start, it is visually sumptuous, but in black and white. If you remade it in colour, you could never match the starkly beautiful composition of the shots, shy Lisa (Joan Fontaine) constantly framed in windows or peeping round doors, the dense black of monochrome night. The colour, the light, the texture of real life, would clutter up your eye and distract you from the lovely artifice. But why not? Well, for a start, it is visually sumptuous, but in black and white. If you remade it in colour, you could never match the starkly beautiful composition of the shots, shy Lisa (Joan Fontaine) constantly framed in windows or peeping round doors, the dense black of monochrome night. The colour, the light, the texture of real life, would clutter up your eye and distract you from the lovely artifice. \"])\n",
    "preds = model_2.predict(token)\n",
    "print(f'{preds[0]} of positivity')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "'it is a glorious film, but you could not make it now. and that is not just my opinion my preview screening companion and fellow lover of weepy old black and white films agreed that not even the director max ophuls could get away with this 1948 classic if he tried to make it in 2009'"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"it is a glorious film, but you could not make it now. and that is not just my opinion my preview screening companion and fellow lover of weepy old black and white films agreed that not even the director max ophuls could get away with this 1948 classic if he tried to make it in 2009 but why not well for start it is visually sumptomps but in black and white if you remade it in clour you can never mathc But why not? Well, for a start, it is visually sumptuous, but in black and white. If you remade it in colour, you could never match the starkly beautiful composition of the shots, shy Lisa (Joan Fontaine) constantly framed in windows or peeping round doors, the dense black of monochrome night. The colour, the light, the texture of real life, would clutter up your eye and distract you from the lovely artifice. But why not? Well, for a start, it is visually sumptuous, but in black and white. If you remade it in colour, you could never match the starkly beautiful composition of the shots, shy Lisa (Joan Fontaine) constantly framed in windows or peeping round doors, the dense black of monochrome night. The colour, the light, the texture of real life, would clutter up your eye and distract you from the lovely artifice. \""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}